{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from source.config import Config\n",
    "config = Config(filename=\"config/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = config.S3_BUCKET  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))\n",
    "\n",
    "# run in local_mode on this machine, or as a SageMaker TrainingJob\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = 'local'\n",
    "else:\n",
    "    instance_type = \"ml.c5.xlarge\"\n",
    "    \n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"Using IAM role arn: {}\".format(role))\n",
    "# only run from SageMaker notebook instance\n",
    "if local_mode:\n",
    "    !/bin/bash ./setup.sh\n",
    "cpu_or_gpu = 'gpu' if instance_type.startswith('ml.p') else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a descriptive job name \n",
    "job_name_prefix = 'HPO-pdm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {'Name': 'Epoch', 'Regex': 'Epoch: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'train_loss', 'Regex': 'Train loss: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'train_acc',  'Regex': 'Train acc: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'train_auc',  'Regex': 'Train auc: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'test_loss', 'Regex': 'Test loss: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'test_acc', 'Regex': 'Test acc: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'test_auc', 'Regex': 'Test auc: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using dataset {}\".format(config.train_dataset_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "key_prefix='fpm-data'\n",
    "training_data = S3Uploader.upload(config.train_dataset_fn, 's3://{}/{}'.format(s3_bucket, key_prefix))\n",
    "testing_data = S3Uploader.upload(config.test_dataset_fn, 's3://{}/{}'.format(s3_bucket, key_prefix))\n",
    "\n",
    "print(\"Training data: {}\".format(training_data))\n",
    "print(\"Testing data: {}\".format(testing_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "max_jobs = 20\n",
    "max_parallel_jobs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'lr': ContinuousParameter(1e-5, 1e-2),\n",
    "    'batch_size': IntegerParameter(16, 256),\n",
    "    'dropout': ContinuousParameter(0.0, 0.8),\n",
    "    \n",
    "    'fc_hidden_units': CategoricalParameter([\"[256, 128]\", \"[256, 128, 128]\", \"[256, 256, 128]\", \"[256, 128, 64]\"]),\n",
    "    'conv_channels': CategoricalParameter([\"[2, 8, 2]\", \"[2, 16, 2]\", \"[2, 16, 16, 2]\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir='source',\n",
    "                    role=role,\n",
    "                    dependencies=[\"source/dl_utils\"],\n",
    "                    train_instance_type=instance_type,\n",
    "                    train_instance_count=1,\n",
    "                    output_path=s3_output_path,\n",
    "                    framework_version=\"1.5.0\",\n",
    "                    py_version='py3',\n",
    "                    base_job_name=job_name_prefix,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    hyperparameters= {\n",
    "                        'epoch': 5000,\n",
    "                        'target_column': config.target_column,\n",
    "                        'sensor_headers': json.dumps(config.sensor_headers),\n",
    "                        'train_input_filename': os.path.basename(config.train_dataset_fn),\n",
    "                        'test_input_filename': os.path.basename(config.test_dataset_fn),\n",
    "                        }\n",
    "                     )\n",
    "\n",
    "if local_mode:\n",
    "    estimator.fit({'train': training_data, 'test': testing_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name='test_auc',\n",
    "                            objective_type='Maximize',\n",
    "                            hyperparameter_ranges=hyperparameter_ranges,\n",
    "                            metric_definitions=metric_definitions,\n",
    "                            max_jobs=max_jobs,\n",
    "                            max_parallel_jobs=max_parallel_jobs,\n",
    "                            base_tuning_job_name=job_name_prefix)\n",
    "tuner.fit({'train': training_data, 'test': testing_data})\n",
    "\n",
    "# Save the HPO job name\n",
    "hpo_job_name = tuner.describe()['HyperParameterTuningJobName']\n",
    "if \"hpo_job_name\" in config.__dict__:\n",
    "    !sed -i 's/hpo_job_name: .*/hpo_job_name: \\\"{hpo_job_name}\\\"/' config/config.yaml\n",
    "else:\n",
    "    !echo -e \"\\n\" >> config/config.yaml\n",
    "    !echo \"hpo_job_name: \\\"$hpo_job_name\\\"\" >> config/config.yaml    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
