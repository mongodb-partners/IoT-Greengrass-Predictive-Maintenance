{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "\n",
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "from source.visualization.model_visualisation_utils import get_dfs_from_hpt, plot_df_list, get_best_training_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.config import Config\n",
    "config = Config(filename=\"config/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data from your Hyperparameter tuning job\n",
    "Enter your HPO job name to visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_job_name = config.hpo_job_name\n",
    "\n",
    "hpt = HyperparameterTuningJobAnalytics(hpo_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = hpt.training_job_summaries()\n",
    "dfs = get_dfs_from_hpt(summaries, metrics=[\"Epoch\", \"train_auc\", \"train_acc\", \"train_loss\",\n",
    "                                           \"test_auc\", \"test_acc\", \"test_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise all the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_list(dfs, metric_name=\"test_acc\", y_label=\"Test accuracy\", min_final_value=0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise the data from the best training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_job_name, best_job_df = get_best_training_job(dfs, \"test_auc\", \"maximize\")\n",
    "best_job_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(1500/108, 400/108), dpi=108)\n",
    "\n",
    "axs[0].plot(best_job_df[\"Epoch\"], best_job_df[\"test_loss\"], label=best_job_name)\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Test loss\")\n",
    "axs[0].grid(color=\"0.9\", linestyle='-', linewidth=3)\n",
    "\n",
    "axs[1].plot(best_job_df[\"Epoch\"], best_job_df[\"test_auc\"], label=best_job_name)\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Test AUC\")\n",
    "axs[1].grid(color=\"0.9\", linestyle='-', linewidth=3)\n",
    "\n",
    "axs[2].plot(best_job_df[\"Epoch\"], best_job_df[\"test_acc\"], label=best_job_name)\n",
    "axs[2].set_xlabel(\"Epochs\")\n",
    "axs[2].set_ylabel(\"Test Accuracy\")\n",
    "axs[2].grid(color=\"0.9\", linestyle='-', linewidth=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build endpoint for the best training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_session = sagemaker.session.Session()\n",
    "model_artifact = sage_session.describe_training_job(best_job_name)[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "model_artifact = os.path.join(os.path.dirname(model_artifact), \"output.tar.gz\")\n",
    "print(\"Building endpoint with model {}\".format(model_artifact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "model = PyTorchModel(model_data=model_artifact,\n",
    "                     role=role,\n",
    "                     entry_point=\"inference.py\",\n",
    "                     source_dir=\"source/dl_utils\",\n",
    "                     framework_version='1.5.0',\n",
    "                     name=\"fpm-model\"\n",
    "                    )\n",
    "\n",
    "endpoint_instance_type = \"ml.t2.medium\"\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=endpoint_instance_type, endpoint_name='fpm-endpoint')\n",
    "\n",
    "def custom_np_serializer(data):\n",
    "    return json.dumps(data.tolist())\n",
    "    \n",
    "def custom_np_deserializer(np_bytes, content_type='application/x-npy'):\n",
    "    out = np.array(json.loads(np_bytes.read()))\n",
    "    return out\n",
    "\n",
    "predictor.serializer = custom_np_serializer\n",
    "predictor.deserializer = custom_np_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.ones(shape=(1, 20, 2))\n",
    "out = predictor.predict(data)\n",
    "print(\"The predicted output is prob of no failure is {:0.4f}\".format(out[0, 0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
